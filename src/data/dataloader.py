# -*- coding: utf-8 -*-
"""dataloader.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-DGIL78lM4AkGogBJbf0GzRX9u49KQKk

### Setup
"""

#from google.colab import drive, output
#drive.mount('/content/gdrive', force_remount=True)

#!pip install import-ipynb -q
#!pip install transformers -q

#from transformers import AutoTokenizer

#output.clear()

#import import_ipynb

#% cd "gdrive/MyDrive/MT"

"""### Miscellanea imports"""

from src.features.preparator import *
from src.features.expander   import *
from src.features.pipeline   import *

from utils.tokenizers import tokenizers, separators

"""### Std imports"""

import pandas as pd
import csv
pd.options.mode.chained_assignment = None

"""### DataLoader"""

T13_topics_filename  = './data/raw/T13/trec2013-ts-topics-test.xml'
T13_matches_filename = './data/raw/T13/matches.tsv'
T13_nuggets_filename = './data/raw/T13/nuggets.tsv'
T13_updates_filename = './data/raw/T13/updates.tsv'

T14_topics_filename  = './data/raw/T14/trec2014-ts-topics-test.xml'
T14_matches_filename = './data/raw/T14/matches.tsv'
T14_nuggets_filename = './data/raw/T14/nuggets.tsv'
T14_updates_filename = './data/raw/T14/updates_sampled.tsv'

T15_topics_filename  = './data/raw/T15/trec2015-ts-topics-test.xml'
T15_matches_filename = './data/raw/T15/matches.tsv'
T15_nuggets_filename = './data/raw/T15/nuggets.tsv'
T15_updates_filename = './data/raw/T15/updates_sampled.tsv'

TOPICS  = "topics"
MATCHES = "matches"
UPDATES = "updates"
NUGGETS = "nuggets"

filenames = {
    "2013": {
        TOPICS:  T13_topics_filename,
        MATCHES: T13_matches_filename,
        NUGGETS: T13_nuggets_filename,
        UPDATES: T13_updates_filename
    },
    "2014": {
        TOPICS:  T14_topics_filename,
        MATCHES: T14_matches_filename,
        NUGGETS: T14_nuggets_filename,
        UPDATES: T14_updates_filename
    },
    "2015": {
        TOPICS:  T15_topics_filename,
        MATCHES: T15_matches_filename,
        NUGGETS: T15_nuggets_filename,
        UPDATES: T15_updates_filename
    }
}

class DataLoader:
    """
    DataLoader class.

    The DataLoader class has the purpose of retrieving and processing data from
    the 2013, 2014 and 2015 versions of the TREC Temporal Summarization datasets.

    This class is intended to be used only during the data generation phase, in which the processed datasets are stored.

    Attributes
    ----------
    datasets : list
        A list containing the names of the dataset that will be returned.
        Supported names are "2013", "2014", "2015".
        Ex: datasets = ["2013", "2014", "2015"]
    only_relevant : bool
        If True, returns only the portion of the data containing relevant text.
        Otherwise, returns the whole data, containing relevant and not-relevant text.
    tokenizer_name : str
        A string representing the name of the tokenizer that will be used to encode the data.
        Supported names are "spanbert", "bert" and "roberta".
    tokenizer_type : str
        A string representing the type of the tokenizer that will be used to encode the data.
        Supported names are "cased" and "uncased".
    max_num_words : int
        An integer representing the max number of words
        that will be contained in a single row of the dataframe returned.
    binary_masks : bool
        If True, the labels referred to the data will be binary (eg. [0,1,1,1,1...,0,0,1,1,1,0]).
        Otherwise, the labels referred to the data will be not-binary (eg. [0,1,2,2,2,3,0,1,2,2,2,2,2,3,0,0,0,4,0,0]).
    contextual : bool
        If True, during the encoding phase of the data, the context will be added to the text.
        If False, only the text data will be encoded.

    Methods
    -------
    load_data()
        Load the data by means of the attributes defined during the initialization of the DataLoader instance.
    """

    def __init__(
        self,
        datasets:       list,
        only_relevant:  bool = False,
        tokenizer_name: str  = "bert",
        tokenizer_type: str  = "uncased",
        max_num_words:  int  = 512,
        binary_masks:   bool = True,
        contextual:     bool = False
    ):
        self.datasets       = datasets
        self.only_relevant  = only_relevant
        self.tokenizer_name = tokenizer_name
        self.tokenizer_type = tokenizer_type
        self.max_num_words  = max_num_words
        self.binary_masks   = binary_masks
        self.contextual     = contextual

        self.lower_text_data = True if self.tokenizer_type == "uncased" else False

        self.tokenizer = AutoTokenizer.from_pretrained(tokenizers[tokenizer_name][tokenizer_type], truncation = True, do_lower_case = self.lower_text_data)
        pass


    def load_data(self) -> pd.DataFrame:
        """
        Load, prepare, encode and adjust the data.
        """

        df = self.__load_datasets()
        df = self.__prepare_data(df)
        df = self.__encode_data(df)
        df = self.__adjust_data(df)
        output.clear()
        return df


    def __load_datasets(self) -> pd.DataFrame:
        """
        Load and concatenate datasets.
        """

        for idx, dataset in enumerate(self.datasets):
            df = self.__load(dataset)

            if idx == 0:
                merged_df = df
            else:
                merged_df = pd.concat([merged_df, df])

        return merged_df

    def __load(
        self,
        dataset: str
    ) -> pd.DataFrame:

        topics, matches, nuggets, updates = self.__read_data(dataset)
        df = self.__load_relevant_updates(matches, updates, nuggets)

        return df

    @staticmethod
    def __read_data(dataset: str):
        topics_filename  = filenames[dataset][TOPICS]
        matches_filename = filenames[dataset][MATCHES]
        nuggets_filename = filenames[dataset][NUGGETS]
        updates_filename = filenames[dataset][UPDATES]

        topics  = read_xml(topics_filename)
        matches = pd.read_table(matches_filename, quoting=csv.QUOTE_NONE)
        nuggets = pd.read_table(nuggets_filename, quoting=csv.QUOTE_NONE)
        updates = pd.read_table(updates_filename, quoting=csv.QUOTE_NONE)

        topics, matches, nuggets, updates = process_data(topics, matches, nuggets, updates)

        return topics, matches, nuggets, updates

    def __load_relevant_updates(
        self,
        matches_df: pd.DataFrame,
        updates_df: pd.DataFrame,
        nuggets_df: pd.DataFrame
    ) -> pd.DataFrame:
        return matches_df\
          .merge(updates_df, left_on='update_id', right_on='update_id')\
          .merge(nuggets_df, left_on='nugget_id', right_on='nugget_id')[["update_text", "match_start", "match_end","query"]]

    def __load_not_relevant_updates(
        self,
        matches_df: pd.DataFrame,
        updates_df: pd.DataFrame
    ) -> pd.DataFrame:

        update_ids = set(updates_df.update_id.unique())
        relevant_updates = set(matches_df.update_id.unique())
        not_relevant_updates = update_ids.difference(relevant_updates)

        not_relevant_updates_df = updates_df[updates_df.update_id.isin(not_relevant_updates)]
        not_relevant_updates_df = not_relevant_updates_df[['update_text', 'query']]
        not_relevant_updates_df['mask'] = [0] * self.max_num_words

        return not_relevant_updates_df

    def __prepare_data(
        self,
        df: pd.DataFrame
    ) -> pd.DataFrame:

        if self.lower_text_data:
            df = lower_text_data(df)

        df = preprocess_text(df)
        df = align_spans(df)
        df = mask_spans(df, self.max_num_words, self.binary_masks)
        df = collapse_data(df)

        if not self.only_relevant:
            df = self.concat_not_relevant_data(df)

        return df

    def concat_not_relevant_data(self, df: pd.DataFrame):
        for dataset in self.datasets:
            topics, matches, nuggets, updates = self.__read_data(dataset)
            not_relevant_df = self.__load_not_relevant_updates(matches, updates)
            df = pd.concat([df, not_relevant_df])

        return df

    def __encode_data(
        self,
        df: pd.DataFrame
    ) -> pd.DataFrame:

        print(f"> Contextual: {self.contextual}")
        print(f"> Lower case: {self.lower_text_data}")

        def encode_plus(row):
            text  = row["text"]
            query = row["query"]

            if self.contextual:
                text = text + separators[self.tokenizer_name] + query

            inputs = self.tokenizer.encode_plus(
                text,
                max_length            = self.max_num_words,
                padding               = "max_length",
                return_token_type_ids = False
            )

            return inputs["input_ids"], inputs["attention_mask"]

        df[["input_ids", "attention_mask"]] = df.apply(lambda row: encode_plus(row), axis=1, result_type='expand')

        df = drop_rows_exceeding_max_len(df, self.max_num_words)

        return df

    def __adjust_data(
        self,
        df: pd.DataFrame
    ) -> pd.DataFrame:

        df[['text', 'mask']] = df.apply(fix_first_character_if_space, axis=1, result_type="expand")

        if not self.binary_masks:
          df['mask'] = df['mask'].apply(fix_consecutive_masks)

        return df