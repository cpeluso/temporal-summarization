# -*- coding: utf-8 -*-
"""datagenerator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_rXlmDTsbMH8fQT9hmUsX7tck4YKwLy8
"""

#from google.colab import drive, output
#drive.mount('/content/gdrive', force_remount=True)
#% cd "gdrive/MyDrive/MT"

#!pip install import-ipynb -q
#!pip install transformers -q

#import import_ipynb
#output.clear()

from src.data.connector import load_data

def get_dataset_path(
    dataset:        list,
    tokenizer:      str,
    tokenizer_type: bool,
    binary:         bool,
    context:        bool,
    only_relevant:  bool
) -> str:

    data = ""

    for name in dataset:
        data = data + name + "_"

    data = data[:-1]

    path = f"data/processed/{data}_{tokenizer}-{tokenizer_type}"
    
    binary_str = "_binary_" if binary else "_not-binary_"
    path = path + binary_str

    context_str = "contextual_" if context else "not-contextual_"
    path = path + context_str

    only_relevant_str = "only_relevant" if only_relevant else "full"
    path = path + only_relevant_str

    return path + ".csv"


def generate_csv(
    dataset_path:   str,
    dataset:        list,
    tokenizer_name: str,
    tokenizer_type: str,
    binary:         bool,
    context:        bool,
    only_relevant:  bool
):

    data = load_data(dataset, only_relevant, tokenizer_name, tokenizer_type, binary, context, test = True)
    data.to_csv(dataset_path, index = False)
    return


def generate():
  dataset = ["2013", "2014", "2015"]

  tokenizers_params = {
      "bert-uncased": {
          "tokenizer": "bert",
          "tokenizer_type": "uncased"
      },
      "bert-cased": {
          "tokenizer": "bert",
          "tokenizer_type": "cased"
      },
      "spanbert-cased": {
          "tokenizer": "spanbert",
          "tokenizer_type": "cased"
      },
      "roberta-uncased": {
          "tokenizer": "roberta",
          "tokenizer_type": "uncased"
      },
      "roberta-cased": {
          "tokenizer": "roberta",
          "tokenizer_type": "cased"
      }
  }

  binary             = [True, False]
  context            = [True, False]
  only_relevant_data = [True, False]

  n_datasets = 0

  for _ in tokenizers_params:
      for _ in binary:
          for _ in context:
              for _ in only_relevant_data:
                  n_datasets += 1


  idx = 1
  for key in tokenizers_params:
      value        = tokenizers_params[key]
      tokenizer      = value["tokenizer"]
      tokenizer_type = value["tokenizer_type"]
      for binary_classes in binary:
          for contextual in context:
              for only_relevant in only_relevant_data:

                  path = get_dataset_path(dataset, tokenizer, tokenizer_type, binary_classes, contextual, only_relevant)

                  print("* * * * * * * * * * * * * * * * * * * * * * * *")
                  print()
                  print(f"({idx}/{n_datasets})")
                  print()
                  print("Generating .csv dataset for params:")
                  print(f"Dataset:        {dataset}")
                  print(f"Encoder:        {tokenizer}-{tokenizer_type}")
                  print(f"Binary classes: {binary_classes}")
                  print(f"Context:        {contextual}")
                  print(f"Only relevant:  {only_relevant}")
                  print()
                  print("- - - - - - - - - - - - - - - - - - - - - - - -")
                  print()
                  print(f"Dataset path:   {path}")
                  print()
                  print("- - - - - - - - - - - - - - - - - - - - - - - -")
                  generate_csv(path, dataset, tokenizer, tokenizer_type, binary_classes, contextual, only_relevant)
                  print()
                  print("Done!")
                  print()
                  print("* * * * * * * * * * * * * * * * * * * * * * * *")
                  print()

                  idx += 1

  return

#generate()